<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[误差反向传播（error backpropagation）推导过程【待完成】]]></title>
    <url>%2Fp%2F55aa0f10%2F</url>
    <content type="text"><![CDATA[误差反向传播算法（error backpropagation）推导过程是每个做深度学习（deep learning）的人员必会的基本功。这里是针对前馈神经网络（Feed Forward Neural Network）或者叫多层感知机（Multi Layer Perceptron）或者叫深度神经网络（Deep Neural Network）的做一个推导。此处用到的误差函数是均方误差（Mean Squared Error）。 1. 前馈神经网络简介基本网络如图1所示，这是一个基本的神经网络，下面来解析这个网络结构。 首先，我们构造输入变量$x_1,…,x_D$的线性组合，形式为： $$\begin{equation}{a_j} = \sum_{i=1}^D {w_{ji}}^{(1)} \cdot x_i + {w_{j0}}^{(1)}\end{equation} \tag{1}$$ 其中$j=1,…,M$，M是输出的总量，且上标(1)表示对应的参数是神经网络的第一层。${w_{ji}}^{(1)}$叫做权重（weight），参数$w_{j0}$叫做偏置（bias）,$a_j$叫做激活(activation)。每个激活都使用一个非线性激活函数(activation function) $h(·)$进行变换，得： $$\begin{equation}z_j = h(a_j)\end{equation} \tag{2}$$ 非线性函数$h(a_j)$通常选用sigmoid，tanh，relu等。$z_j$为经过激活函数的值。这些值会再次线性组合，得到输出单元激活（output unit activation）： $$\begin{equation}{a_k} = \sum_{i=1}^K {w_{kj}}^{(2)} \cdot z_j + {w_{k0}}^{(2)}\end{equation} \tag{3}$$ 这个变换对应于神经网络第二层，类似于之前。最后使用一个恰当的激活函数进行变换，得到神经网络的一组输出$y_k$。对于标准回归问题，激活函数是恒等函数，从而$y_k = a_k$。对于一个二元分类问题，每个输出单元激活可以使用logistic sigmoid进行变换，即: $$\begin{equation}y_k = \sigma(a_k)\end{equation} \tag{4}$$ $\sigma(·)$函数形式这里不再给出，可以自己查看。对于多分类问题，可以使用softmax激活函数。 最后综合以上的观点，我们容易得到一个网络整体，如下: $$\begin{equation}y_k = \sigma(\sum_{i=1}^K {w_{kj}}^{(2)} \cdot h(\sum_{i=1}^D {w_{ji}}^{(1)} \cdot x_i + {w_{j0}}^{(1)}) + {w_{k0}}^{(2)})\end{equation} \tag{5}$$ 其中所有权重参数和偏置被聚集在一起，记做$\vec{w}$。因此，神经网络可以简单地认为是从输入变量$\vec{x}$到输出变量$\vec{y}$的非线性函数，映射由调节参数$\vec{w}$来控制。 具体网络结构正如图1所示。计算公式（5）的过程可以认为是信息通过网络的前向传播（forward propagation）。 通过定义额外的输入变量$x_0$讲公式(1)中的偏置参数整合进权重参数集合中，并且$x_0$被限定为1，因此公式（1）可以改写为： $$\begin{equation}{a_j} = \sum_{i=0}^D {w_{ji}}^{(1)} \cdot x_i\end{equation} \tag{6}$$ 第二层的偏置也做类似处理，最终整体网络函数可以从公式（5）变为： $$\begin{equation}y_k = \sigma(\sum_{i=0}^K {w_{kj}}^{(2)} \cdot h(\sum_{i=0}^D {w_{ji}}^{(1)} \cdot x_i))\end{equation} \tag{7}$$ 至此，一个简单的前馈神经网络已经完成了。 2. 误差反向传播算法简介我们的目标是寻找一种计算前馈神经网络的误差函数$E(w)$的梯度高效的方法。我们将会看到，可以使用局部信息传递的思想来完成这一点。在局部信息传递的思想中，信息在神经网络中交替的向前、向后传播。这种方法被称为误差反向传播（error propagation），有时被称为backprop，通常简称bp算法。 关于训练过程的本质。大部分训练算法涉及到一个迭代的步骤用于误差函数的最小化，以及通过一系列的步骤进行权重调节。在每一个这样迭代过程中，我们可以区分这两个不同的阶段。在第一个阶段，误差函数关于权重的导数必须被计算出来。正如我们稍后看到的那样，反向传播算法的一个重要贡献是提供了计算这些导数的一个高效的方法。由于正是这个阶段，误差通过网络进行传播，因此我们将专门使用反向传播这个属于来描述计算导数的过程。在第二个阶段，导数用于计算权重的调整量。最简单的方法，也是最开始由Rumelhart et al.（1986）考虑的方法，涉及到梯度下降。认识到这两个阶段属于不同的阶段是很重要的。因此，第一阶段，即为了计算导数而进行的误差在网络中的反向传播阶段，可以应用于许多不同种类的网络，而不仅仅是多层感知器。它也可以 应用于其他的误差函数，而不仅仅是简单的平方和误差函数。它也可以用于计算其他类型的导数。第二阶段，即使用计算过的导数调整权重的阶段，可以使用许多最优化方法处理，许多最优化方法本质上要比简单的梯度下降更强大。 3. 简单线性模型误差导数的计算许多实际应用中使用的误差函数，例如针对一组独立同分布的数据的最大似然方法定义的误差函数，由若干项的求和公式组成。每一项对应于训练集中的一个数据点，即： $$\begin{equation}E(w) = \sum_{n=1}^N {w}\end{equation} \tag{8}$$ 这里，我们要考虑计算$\nabla E_n(w)$的问题。这可以直接使用顺序优化的方法计算，或者使用批处理方法在训练集上进行累加。 对于一个特定的输入模式n（等价于对于一个特定的样本$\vec{x}$），误差函数形式为： $$\begin{equation}E_n = \frac{1}{2} \cdot \sum_{k} {(y_{nk}-t_{nk})}^2\end{equation} \tag{9}$$ 其中$y_{nk} = y_k(\vec{x_n}, \vec{w})$，$y_{nk}$表示对于特定输入模式的输出，$t_{nk}$表示对应的实际标签（或预测值）。这个误差函数关于权重$w_{ji}$的梯度为： $$\begin{equation}\frac{E_n}{w_{ji}} = (y_{nj} - t_{nj})x_{ni}\end{equation} \tag{10}$$ 它可以表示为与链接$w_{ji}$的输出端关联的“误差信号”$y_{nj}-t_{nj}$和与链接的输入端相关连的变量$x_{ni}$的乘积。我们现在会看到这个简单的结果如何扩展到更复杂的多层前馈神经网络中。 4. 通用前馈神经网络bp算法推导 5. 一个简单的例子参考资料： BISHOP, C, M. Pattern recognition and machine learning[M]. New York:Springer, 2006. 225-244]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>AI一丝一毫</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 3 读取中文文件]]></title>
    <url>%2Fp%2F6821aaf2%2F</url>
    <content type="text"><![CDATA[在windows环境下，python3 读取中文文件方法，折腾了很久，总结了下。 先找到文件编码格式。 因为读文件的时候，需要知道文件的编码格式，编码格式怎么看呢？ 我的方法是，使用windows自带的记事本打开文件，然后ctrl+shift+s，或文件-另存为，可以看到文件的编码格式。 常见的编码格式有utf-8，gbk，gb2312。我今天把这些编码格式都试了发现都不能解码，最后发现文件编码格式是Unicode big endian，然后看到Stack Overflow上说，编码就是UTF-16[1]，最终解决问题。 编写相应代码。 两种方法，方法一，使用codecs模块： 123import codecswith codecs.open(file, 'r', encoding='utf-16') as f: text = f.read() 或 12with open(file, 'rb') as f: text = f.read().decode('utf-16') 方法二： 12with open(file, 'r', encoding='UTF-16') as f: text = f.read() 自己写的时候记得修改encoding为自己文件的相应格式，同时根据自己需要修改读写。 参考资料： [1]. https://stackoverflow.com/questions/8827419/converting-utf-16-utf-8-and-remove-bom]]></content>
      <categories>
        <category>experience of code</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A consolidated perspective on multi-microphone speech enhancement and source separation 阅读笔记（1）【待写】]]></title>
    <url>%2Fp%2F9c92dc77%2F</url>
    <content type="text"><![CDATA[论文A consolidated perspective on multi-microphone speech enhancement and source separation阅读分享]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经济学人（The Economist）EPUB，MOBI,，PDF及MP3分享下载]]></title>
    <url>%2Fp%2F1c573a7b%2F</url>
    <content type="text"><![CDATA[简介《经济学人》是一份由伦敦经济学人报纸有限公司出版的杂志，创办于1843年9月，创办人詹姆士·威尔逊。杂志的大多数文章写得机智，幽默，有力度，严肃又不失诙谐，并且注重于如何在最小的篇幅内告诉读者最多的信息。该杂志又以发明巨无霸指数闻名，是社会精英必不可少的读物。该杂志英文电子版可通过移动App、网站或者有声版阅读每周完整内容。 杂志主要关注政治和商业方面的新闻，但是每期也有一两篇针对科技和艺术的报导，以及一些书评。杂志中所有文章都不署名，而且往往带有鲜明的立场，但又处处用事实说话。主编们认为：写出了什么东西，比出自谁的手笔更重要。从2012年1月28日的那一期杂志开始《经济学人》杂志开辟了中国专栏，为有关中国的文章提供更多的版面。 下载链接及说明内容格式含EPUB，MOBI,，PDF及MP3。点进日期进去就到了百度网盘链接，输入后面的密码就可以了。本内容会持续更新。仅供学习交流使用，请勿用作商业用途。 20180505 密码: ygdx。 20180428 密码：8v3d。 20180421 密码：kdpn。 20180414 密码: u2xa。 20180407 密码: z44r。 20180331 密码: yjr9。 20180324 密码: j737。 20180317 密码：8q9i。 20180310 密码：n5mv。 20180303 密码：hnij。 2018年1月-4月、2017年全年、2016年全年下载链接2018年全年经济学人下载（含EPUB、MOBI、PDF、MP3） 密码: ihqw。 2017年全年经济学人下载（含EPUB、MOBI、PDF、MP3） 密码: 57z5。 2016年全年经济学人下载（含EPUB、MOBI、PDF、MP3） 密码: v2bd。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>经济学人</tag>
        <tag>The Economist</tag>
        <tag>下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine-learning-yearning(by Andrew Ng) 1-22章翻译最新版]]></title>
    <url>%2Fp%2Fb706c49c%2F</url>
    <content type="text"><![CDATA[machine-learning-yearning本内容是Andrew NG的My Machine Learning Yearning 1-22章内容的翻译及其原稿，会持续更新。 Andrew表示最近每周会持续更新本书籍，我决定把它翻译出来，加深自己的印象，同时希望能对大家有点帮助，后续会持续更新。 本书官网：http://www.mlyearning.org/ Andrew Ng关于本书介绍亲爱的朋友，你是如何组织一个人工智能（AI）项目的呢？ 人工智能（AI），机器学习（Machine Leaning）和深度学习（Deep learning）正在改变众多行业。我一直在写此书——Machine Learning Yearning，来教你如何构建机器学习项目。 本书的重点不在于教授机器学习算法，而在于使机器学习算法发挥作用。一些人工智能技术会给你一个锤子，而本书教你如何使用锤子。如果你渴望成为人工智能技术的领导者并想学习如何为你的团队设定方向，这本书将会有所帮助。 阅读完Machine Learning Yearning之后，你将能够： 为人工智能项目最有前途的方向设立优先级 诊断机器学习系统中的错误 在复杂设置中构建机器学习，例如不匹配的训练集/测试集 建立一个可以人类比较或超越人类表现的机器学习项目 了解何时以及如何应用端到端学习（end-to-end learning），迁移学习（transfer learning）和多任务学习（multi-task learning） 从历史上看，学习如何制定这些“策略”决策的唯一方法是在研究生课程或公司中做多年的学徒。我正在写的Machine Learning Yearning可以帮你快速的获得这项技能，以便你可以更好地构建人工智能系统。 本书大约100页，包含很多易于阅读的1-2页的章节。如果你希望收到每章完成后的草稿，请注册邮件列表。 —— 吴恩达（Andrew Ng） 翻译版gitbook阅读地址：gitbook阅读体验更好，欢迎来点击下面链接阅读翻译版： https://yuchenchen.gitbook.io/machine-learning-yearning/ 翻译版github地址：https://github.com/yucc2018/machine-learning-yearning 英文原版pdf下载：原文原版手稿已经更新到19章，下载链接分别如下: 1-14章pdf下载（2018.04.18版） 15-19章pdf下载（2018.04.25版） 20-22章pdf下载（2018.05.02版） 我自己手动将上面两部分合并，成为1-19章的合集。想下载一个的直接点下面这个链接。 1-22章合并版pdf下载（2018.05.02更新） 翻译章节：1. 为什么是机器学习策略？ 2. 如何使用本书来帮助你的团队 3. 预备知识和表示符号 4. 规模推动机器学习进度 下一部分：设置开发和测试集 5. 你的开发集和测试集 6. 你的开发集和测试集应当来自相同的分布 7. 开发集和测试集需要多大？ 8. 为你的团队进行算法优化建立一个单数字估指标 9. 优化和满足指标 10. 使用开发集和评估指标来加速迭代 11. 何时更改开发集/测试集和评估指标 12. 小结：设置开发集和测试集 下一部分：基本错误分析 13. 快速构建你的第一个系统，然后迭代 14. 错误分析：查看开发集的例子来评估想法 15. 在错误分析中并行评估多个想法 16. 清理错误标记的开发集和测试集样例 17. 如果你有一个很大的开发集，将它分成两个，而且你只看一个 18. 眼球开发集和黑盒开发集应该多大？ 19. 基本错误分析 下一部分：偏差（Bias）和方差（Variance） 20. 偏差（Bias）和方差（Variance）：错误的两大来源 21. 偏差（Bias）和方差（variance）的例子 22. 对比最优错误率 待Andrew更新后翻译章节： Addressing Bias and Variance Bias vs. Variance tradeoff Techniques for reducing avoidable bias Techniques for reducing Variance Error analysis on the training set Diagnosing bias and variance: Learning curves Plotting training error Interpreting learning curves: High bias Interpreting learning curves: Other cases Plotting learning curves Why we compare to human-level performance How to define human-level performance Surpassing human-level performance Why train and test on different distributions Whether to use all your data Whether to include inconsistent data Weighting data Generalizing from the training set to the dev set Addressing Bias, and Variance, and Data Mismatch Addressing data mismatch Artificial data synthesis The Optimization Verification test General form of Optimization Verification test Reinforcement learning example The rise of end-to-end learning More end-to-end learning examples Pros and cons of end-to-end learning Learned sub-components Directly learning rich outputs Error Analysis by Parts Beyond supervised learning: What’s next? Building a superhero team - Get your teammates to read this Big picture Credits 联系方式：6506666@gmail.com]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>AI一丝一毫</tag>
        <tag>machine learning yearning</tag>
        <tag>Andrew Ng</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017年算法工程师（机器学习方向）找实习找校招经验总结]]></title>
    <url>%2Fp%2F102faecd%2F</url>
    <content type="text"><![CDATA[写在前面，这是我去年年底（2017.12）依据自己找实习找工作经历撰写，分享在北邮人bbs上的文章，觉得还是有价值的，重新发表在这里。 2017年即将过去，今年是忙碌的一年，从2月17还没开学就来学校，到现在12月初，工作的事情终于尘埃落定，现将这一段经历写下来，希望能帮助来年找工作的学弟学妹，同时，对自己来说，也是一年的工作总结，总结经验教训，对自己一个提升。 简单介绍一下，北邮本硕，要找的岗位主要是算法工程师（机器学习方向）。算法工程师找实习的时候，最终的结果是找到了四家：亚马逊、京东、滴滴、腾讯地图。找工作的时候，主要有百度sp、滴滴sp、美团sp、新浪sp、搜狗sp、小米（没谈薪资直接拒了）等。总体而言，今年找工作相对以往还是很顺利的，各大公司都很缺人，各家开出的工资相对于去年都非常诱人，找工作的情况也出乎了我们很多人的意料。 年初的时候，大概2月28左右开学吧，我2月16就来了，开始准备找实习。简单的说一下面试的情况，有些可能已经忘记，现在说个大概情况。 各家面试情况 滴滴，实习的是在论坛招人内推的，两面，第一面是主要问了svm lr等原理，简历上的项目，svm怎么用于多分类。第二面写了一道算法题，leetcode 63. Unique Paths II。问我是否会用spark，我说不会。然后就给我介绍了他们所做的项目。后来回来的路上我想了想，当时问我是否会不会spark，我应该说我虽然不会，但是可以学的，留个好印象。没过多久，就发了实习offer。后来去了滴滴实习。实习转正三面，每面半小时，所以还是相对容易了很多。转正一面，主要问了在滴滴的实习，给我的柑橘是更偏重业务，对技术并不是太热衷，做的项目的业务的主要应用是什么，将来怎么评价之类的。一道算法题，leetcode上的，旋转数组，leetcode 59. Spiral Matrix II。转正二面，也是问了问滴滴的实习，问了下gbdt的原理，xgboost与gbdt的区别，gbdt用于分类时，分类概率的梯度体现在哪里。一道编程题，矩阵A与矩阵B相乘得到矩阵C，给定A和B，求C的秩。转正三面，三面面试官是一个研究员，对数学推导有独特的兴趣。问了svm的推导，什么是凸函数，为什么拉格朗日对偶方程成立。滴滴的offer大约10月初发。 美团。实习的时候貌似没内推，走的校招。一面，上来让写了几个算法，一个是数组全排列，一个是二叉树的非递归先序遍历，一个是反转链表。然后问了过拟合问题，l1 l2正则区别之类的。然后进入二面，二面的时候面试官先问了一个二叉树的垂直遍历，LeetCode 314. Binary Tree Vertical Order Traversal（这个题是leetcode付费的题）。没答上来，就挂了。校招时是找斜对门的美团实习的大佬内推的。共三面，一面面完告知通过接着二面，二面面完告知通过让回去等三面，三面是电话面，晚上7点三面的，晚上12点就收到录取意向书。做完笔试，过了几天，晚上9点多美团打电话过来，说明天早上10点来某某酒店面试。第二天一面问了简历上的项目，编程题问了1到n的一个排好序的数组，少了一个元素，怎么找出来。我说二分查找，时间效率logn（也可以用位运算，时间效率n）。二面问了项目，我简历上有k-means，就问了其他聚类的方法，距离有多少种，影响聚类的因素。问了编程题leetcode 198 House Robber（动态规划）。三面电话面，问了项目，问了过拟合的问题产生、解决办法，问了两个编程题，leetcode 69 Sqrt(x)，两种方法解决，一种是二分查找，一种是牛顿法。另一个编程题给忘了，想起再补充。 百度。内推实习的时候，去年毕业的师兄去了百度组，我们就找他内推的。说实话，挺后悔的，我并不喜欢他这个方向，一方面我不擅长，另一方面过了我也不会去。不该图省事找他推的。内推实习时，共二面，挂了。面试主要问的是方面的，也没什么意思，答的也不好。问了leetcode 206 Reverse Linked List，让自己定义链表，问指针，对象，指针是对象吗，对C++的考察很多。到校招实习时，没投简历。到校招开始时，我投的比较晚，所以错过了内推机会，然后到了校招。校招共三面，都是技术面，当天状态不太好，感冒了。主要问了二分类的标准有多少种，每种都是什么。struct与class的区别。线程与进程的区别，io密集，计算密集使用多线程还是多进程。一个发生器，产生0的概率是p，产生1的概率是1-p，p！=0.5，怎么使用这个产生0和1等概率的发生器（可以每次产生两个数字，0 1判为0，1 0判为1，其他的舍弃重来）。k-means聚类相关问题，svm与lr的对比与区别。编程题问了多道，最小栈leetcode 155. Min Stack，还有几道其他题目，忘了已经。 腾讯。实习投的北京微信，校招投的广州微信，然而都没找我面试。找实习时，qq空间把我捞了起来邀请我面试，我拒绝了，后来腾讯地图邀请我去面试，就去了。面试一下午，从1：40到5：20，共四面，前三个为技术面，第四个为leader面，leader也问了一些技术，未来的规划，来这里之后会干什么，反正就是泛泛的聊，还有就是你印象最深刻的一件事，最感动的，最难忘的一件事，哎，这些问题。先是40分钟一套卷子，让做题，5道编程题，3道问答题可以选择做。我当时40分钟把5道编程写完就没时间了。有一道是leetcode 300 Longest Increasing Subsequence，其他忘了，不过也不难。面试聊了项目，花了不少时间，也看了那5道编程题，怎么处理POI中的名称问题，就是说一个地点可能有很多备选的名字，怎么确定哪个更合适。爬虫相关技术。还聊了聊手头的offer情况等。到后面校招内推时，投的广州微信，但是广州微信并不怎么缺人，投的人又多，所以就没给面试机会。qq音乐把我捞了起来，但我并不想去深圳，就拒绝了。再后来校招的时候，不知道腾讯哪个部门邀请我去面试开发岗。我想了想，我并不想做开发，就拒绝了。现在看起来，腾讯我还是有很多机会的，然而我都拒绝了。哎，现在仔细想一想，虽然房价高，但是深圳其他方面都挺好的，空气质量好，各种生活还是很不错的，四个面试机会，只面了一个，其他的都自己拒绝的，感觉还是非常可惜的。 新浪。实习没有投，校招投的内推，面了四面，两面技术面，三面总监面，四面hr面。主要问了项目、gbdt与xgboost的区别、贝叶斯学派和统计学派的分歧，编程题问了反转链表leetcode206，top k大的数（剑指offer题目，第一版30题）。 搜狗。实习没有投，校招内推面了两面挂了，校招面了两面，发了offer。校招内推时，也是为了省事，找的搜狗某组的大大大师兄推的，跟百度那个类似，我也不感兴趣。面试问了项目，编程问了最小编辑距离leetcode 72 Edit Distance，剑指offer 31连续子数组的最大和（找到起始位置和最大和）。校招两面。主要是项目，二分类的评价标准有哪些、分别是什么。编程有二叉树的先序中序后续非递归遍历。二面面完说去看看领导在不在，回来说领导不在，就让回来了，说后面可能有hr面。我心想，这是委婉拒绝的套路吧，过了几天hr让我去公司面，问了下学历，问考研还是保研，有哪些offer，然后发了offer。 小米。三面技术面。面试之后，发了意向书，直到前些日子才联系我。我说我已经交其他公司三方，校招结束了。编程题问了leetcode 228. Summary Ranges， 给一个二叉树，找到最深的一个节点到最浅的一个节点之间的最小公共父节点，我的方法是层次遍历，找到最浅的节点及最深的节点，然后使用 leetcode 236 Lowest Common Ancestor of a Binary Tree中的方法去解决问题，时间效率为o（n），空间效率为o（n），面试官给的方法是保存所有路径，然后去查找。问了项目，两个项目问的非常细，使用random forest计算特征重要性，对该维特征不进行permutation，删除可以不可以。峰度、偏度的数学公式及意义。二分类中AUC的具体定义，各个分类评价方法。最后三面leader面的时候还问了两个思考问题，金融违约率，可能要房贷很长时间才能知道是否违约，数据少，实验样本回收时间长所以对于这个问题怎么处理。还有就是为何利用训练集训练，用来预测测试集可以用来预测。我说的是这是基于一个大的假设，训练样本和测试样本是基于独立同分布的，后来我想了想我觉得也可以从熵 信息增益的角度思考。 京东。 京东是实习过了，校招没邀请我面试。 校招的时候，我找的京东的师兄内推的，后来也参加了校招，笔试的挺好的，就是没邀请面试，也有一些同学跟我一样的情况，可能简历挂了吧。京东实习面试是三面，两面技术面，一面hr面。感觉比较坑的是京东的三面分别安排在了三天，为此我给我们老师请了三天假。京东用的较多的是Java，我自己是cpp Python较多。面试相对简单，问了svm原理推导，两个排好序的数组怎么合并成一个排好序的数组，双指针啊。问了一个业务题场景题，给你一些恶数据，让你对顾客对一个新商品的退单率预测，如果能预测出退单率，则可以延迟或者不发货，节约物流成本。hr面的时候问了几个问题，现在厉害的很多都是博士，你怎么看，有无读博的打算？介绍一个你的项目。 亚马逊。 亚马逊也是实习过了，校招没邀请我。亚马逊实习投的早，后来邀请去面试，共两面技术面。第一面是一个美丽的北邮学姐（我不认识），主要问了简历的项目，lstm的原理，lstm为何能长时记忆。二面有两位面试官同时面，主要是编程题，也问了项目。编程题相对都是比较容易的，主要是leetcode 1 Two Sum ，leetcode 15 3Sum，leetcode 16 3Sum Closest ，leetcode 18 4Sum。还问了一个业务场景预测题，亚马逊要销售很多商品，你如何去预测商品需要用到的货仓的体积。如果能预测出货仓体积，这样子就很容易去租赁仓库了。后面的公司都是实习和校招都没过的。有的是面试未通过，有的是没有面试，也一并写下来。 360。实习三面挂，校招内推一面挂。实习面试，是我第一次去面试，面的实习，路上还想，如果360过了，实习就结束了，可惜想法是美好的，现实是残酷。实习是三面，两面技术，一面hr。上午10点面，40分钟一场，等面完二面等下午1点半还是2点开始hr面。两面技术面的的确挺不好的，那是第一次去面试，也没看网上的面经，自己的技术也并不太好。技术主要问了，svm的原理，smo算法，lstm有什么好处，为何防止梯度爆炸和梯度消失，bp推导，编程题问了树的深度，这个简单，但是代码中竟然把==错写成了=，范了这样的错误。还有问了找出二叉树中和最大的一条路径，不必经过根节点leetcode 124. Binary Tree Maximum Path Sum。 hr面的时候问了以前的实习经历，考研还保研，我说考研，问考了多少分，有无对象，将来是打算在哪里发展。校招的时候，内推去面，一挂面。问了简历项目，问了聚类算法都有那些，算法的比较，cnn的原理各个层的作用，编程题问了二分搜索。感觉面的挺好的，然而还是挂了。很多同学都说360基本不缺人。的确实习还是校招，周围过360的很少，包括很多大牛。 阿里。 实习内推一面视频面，后来的实习校招也是一面挂，校招也是视频一面挂。实习面的时候，主要问了简历项目，问了svm的原理，svm的核函数的作用，lr能不能用核函数，为什么。校招的时候，问了多元高斯函数的期望是什么，怎么推导？熵的定义是什么？怎么理解？编程题是剑指offer 30题，最小的k个数，也是leetcode 215 Kth Largest Element in an Array，写代码并分析时间复杂度。阿里实习投的是阿里妈妈，校招投的是蚂蚁金服。除了技术需要加强之外，视频面试也表现出了两个缺点，一个是表达沟通需要大大的加强，一个是要自信，同学在旁边表示我的表达太快，太紧张了。还有一个非常重要的一点，我校招内推投的太晚了（8.8投的）以至于错过了校招内推的时间点，没人找我面试，直接到了校招了。我为什么投这么晚呢？可以记住一个时间点，大概6月底，7月初就开始互联网的内推了。我们是7月开始放实习，7.4才开始去实习公司报到，那个时候想着，简历上没有个像样的实习，刚去实习的公司，什么也不会呢，简历也不好写。所以想稍微等一等，结果8.8再投的时候，差不多快结束了，我记得校招的内推是8.18左右。所以时间节点一定要把握好，千万不要拖。 网易。实习的时候是笔试挂了，校招内推也是笔试挂了，校招的时候申的深度学习工程师，校招时二面挂。第一面是一个美女姐姐，第二面试是两位面试官一起面。主要问了项目，自己搭建网络的结构，CNN网络的结构，相关知名的cnn网络结构，编程就问了一道，一些坐标点都是（x，y）形式的让以x的大小排序，x一样的按y排序，我说很简单啊，使用sort函数写个compare函数就可以了，不知道是要考察什么，要考察现场写排序算法吗？我当时应该问问的。其实我对深度学习的了解还是太少，需要更多的学习，目前更多的是工程使用上的东西，我当时应该申机器学习工程师的，或许面试就通过了。 网易游戏。 实习的时候简历挂了，校招的时候本来申的人工智能工程师，结果给我转成了游戏研发工程师，还不能更改，我去面了，一面挂。首先可以说明一点，网易和网易游戏是分开招聘的，两家可以分别投。再有就是网易游戏有两块，一块是雷火盘古，一块是互娱。我实习投的是雷火，找了互娱的同学内推了一下，最开始不知道他们内部互不承认，结果相当于没有内推，简历就给我挂了。所以投雷火一定要找雷火的人推，投互娱一定要找互娱的人推，不然等于没有推。还有就是网易游戏实习开始的特别早，去年11月还是12月，就开始内推了，现在也是12月大家可以关注下消息。今年2月份我刚过完年来学校，我同学跟我说他网易游戏已经笔试过两批了，我顿时觉得自己落后了很多，有种别人校招都结束了，你才刚来学校的feel，何况我比学校规定的开学时候早来了十多天，当时一顿慌张，赶紧问问师兄师姐到底什么情况。再有就是校招面试，先是40分钟让写道编程题，然后再去见面试官。因为他们给我调成了开发，我自己也确实没什么经验，问的全是语言细节的，算法工程师从来不怎么问的，结果可想而知，就挂了。 今日头条。实习的时候一面挂，校招的时候也是一面挂。实习的时候问了简历，当时第一家面的360，挂了。貌似第二家面的头条，也挂了，挂了很多家，心情还是挺郁闷的当时。头条实习一面的时候先让写编程题，类似于剑指offer第一版的第7章的7.2里的题目，叫做多叉树（没有指向父节点指针）中两个节点的最低公共祖先。然后问了项目，问了svm的推导，问的非常细，问距离到底如何定义的，不可分的svm中松弛变量怎么理解，如果有个点特别异常，是否还能找到分割面，我自己当时水平也比较菜，他问一个我回答一个，我回答一个，他说一句靠（kao 四声），我心里一阵慌张。问了lr的公式推导。的确面的不好。校招的时候，找的内推，当时内推要内推码，但是内推有内推码也要笔试，但是内推码有一种是内推白金码，只有头条技术员工有，且每人只有一个，有了内推白金码，可以直接面试，免笔试，早面试。我就找人要了一个内推白金码，是头条的第一批面试，挂了。校招内推挂了，不能参加后续的校招，跟360一样的。校招一面，主要问了简历，问了项目，问了一些倒排索引，词向量及复杂度相关问题。编程是leetcode 57 Insert Interval，这是leetcode的hard题目，根据大家的反应，头条编程题问的普遍相对难一些。编程题我没答上来，挂了也是很正常的事情。 海康威视。实习没投，校招投了。共三面。先是电话一面，后是去公司面试二面和三面，而面试技术面2v1，三面是hr面。一面就是简单的聊了聊，问了问。二面是聊聊平时用什么技术，用什么网络，业内（我的研究方向）主要用什么方法，业内（我的研究方向）都在做什么，对了解多少，有无转方向的意愿。也没太多的问题，更多的是发散性的问题。hr面的时候，问了有多少offer了，因为已经很晚了，我就说我现在有7家公司发了sp offer。后来没给我发offer，我觉得可能主要在于我对方向了解甚少，我本来也不是研究那个方向，但是平时还是有很多与方向学习的机会，我都不想去学。这给我一个提示，除了技术的深度，一定要提高的自己涉猎的广度。 微策略。 先是线下笔试，然后三面技术面。微策略是先去听宣讲会，然后再线上面试。笔试面试都是全英的。笔试一小时，题目还是不太难的。面试一面，13*16 = 244，问这事几进制，答：（1x+3)(x+6) = 2x^2 + 4x + 4 =&gt; x=7。证明：n(n^2-1) 对于任何n&gt;=3的奇数都可以整除。答：可以数学归纳法，假定n=2k+1满足条件，证明（2k+3)也满足，最后得到[(2k+1)^3-(2k+1)] + 24k^2 + 48k + 24显然成立。问一个字符串的全排列，我以为是leetcode 46. Permutations，实际上是leetcode 47. Permutations II。我按照46的思路写的代码，主要差别是47是有重复的。后来他提示是否有bug，我想到了47，然后改了重复的部分，但是竟然又忘了先sort下。所以这道题答的不好。这道题答的不好，我也反思过，自己不够灵活，没考虑那么多，直接对上题号了，思想僵（jiang）化了，一定要与自己做过的题目对一起做对比，注意差别。二面。给一段代码，让找出bug，里面涉及了指针和malloc和strcpy这些，我没找出bug。编程题1，一段链表，反转前n个节点。类似于leetcode 92. Reverse Linked List II。编程题2，leetcode 25. Reverse Nodes in k-Group。 编程题3，single number问题leetcode 136. Single Number，编程题4，leetcode 137. Single Number II。三面。编程题1， 121. Best Time to Buy and Sell Stock，编程题2， leetcode 123. Best Time to Buy and Sell Stock III，编程题3，leetcode 122. Best Time to Buy and Sell Stock II，编程题4，397 Integer Replacement 。编程题 2写了两种方法，思考花了些时间。编程题4给了几种解法，但遗憾的是没有给出最优解。 商汤。实习的时候笔试过了，邀请面试，我当时有三四家都要面试，还要交ppt，听说比较难，就没去面。校招的时候笔试挂了。 face++，实习没投，校招笔试挂。 微软。实习和校招都是笔试挂了。 蘑菇街&amp;美丽说。 实习投了没理我。校招内推把我简历挂了，校招又投了一次，又挂了一次简历。 freewheel，第四范式，hulu，快手等投了校招，没消息，应该是简历挂了。看周围人都投了链家网的内推，我就投了链家网，结果链家的校招是线下笔试，我不知道，就错过了。 面试心得以上就是详细的面试部分，下面是一些自己参考别人的经验及自身的经历，总结的一些经验教训吧，希望对自己是一个总结，对学弟学妹们也希望能有所帮助吧。 把握好时间实习、校招的时间表。实习分为内推阶段和校招阶段。众所周知，互联网找工作的时间越来越早。2017年今年的情况是大部分公司在2-5月（实习包含内推和校招两阶段）。先是内推，后面就是校招。也有特别早的，比如网易游戏，去年12月左右就开始内推了。内推阶段刚走完或者没有走完就开始到了实习的校招阶段了。校招也分两个阶段，内推阶段和（正式）校招阶段。时间在6月底左右-11月。很多公司在6月底7月初就开始了，比如京东、阿里、腾讯等。京东以往貌似都是比较早的，今年也是早早的面试，早早地发了offer。除了内推和校招阶段，有的公司貌似还有提前批（京东貌似有）。我自己的教训就是，校招投的太晚，以至于错过了很多面试机会。我为什么投这么晚呢？上面也分析过，有希望能把实习经历写上去，也有自己想好好复习好好准备的因素。实习的时候我投的就非常早，结果准备的不充分，遇到了很多被拒，也算是找实习遇到的小后遗症吧。投的太晚错过了腾讯、阿里的校招内推阶段，是非常可惜的。阿里的校招内推需要重新内推，腾讯的实习的简历会直接转成校招内推的，不用再推，可以更新下自己投的部门信息就行了。我自己也没去更新简历。总之吧，自己的简历不够好看，希望能刷新下简历再投，以至于错过了时间。还有亚马逊，亚马逊写的校招开始时9.10，结果我投的晚，人家9.5就开始第2轮还是第3轮笔试了。挺可惜的错过了。 把握好内推和实习转正机会。现在大家都招内推了，然后内推跟正式的也差不多了。无论从实习还是校招的结果来看，内推会吸收很大一批人，所以留给校招的名额并不多，并且所有人都参加校招，所以竞争是非常激烈的。所以大家一定要把握好内推的机会。尤其校招的时候，很多公司的实习生都转正了，占用了很多名额，所以校招的内推和校招阶段竞争还是非常激烈的，学弟学妹们还是要加油。说是实习转正，大家要千万把握好机会。如果想在某个公司留下，我个人建议，优先去该公司实习。实习转正，尤其是实习的本部门转正是最容易的。想在哪个部门工作，实习是一个非常简单又省事的途径。内推的时候最好要找熟人内推。可以帮忙查看简历进度。腾讯的内部可以看到简历评价S级、A+、A、B、C等。也可以看到面试官的名称等信息。阿里的也可以看到面试官的名称信息，并依此判断某些信息。找熟人推，方便后续的联系。还有最好直接推到自己想去的部门。如果自己有明确的部门，可以直接推到相关部门，如果不写部门，很多都是直接进公司简历池。如果写了，相当于先在部门简历池，然后进公司简历池，相对而言，相当于多了一次机会。找熟人内推的另一个好处是，如果是投的他的部门，他可以直接把简历扔给他老大，邀请你去面试。 写好简历，讲好项目。简历肯定是特别重要的，简历是面试官对你的第一印象，一定要好好写，如实稍微修饰的写。自己的会的能讲清楚明白的都写上去，自己含糊其辞的搞不太清楚容易被问懵的就不要写了。经常更新自己的简历。我自己的简历面试完就会改一改，修一修，每次都总结一下面试，修缮一下简历。简历上的项目的细节之类的自己都要清楚地明白，一定不能给面试官问的答不出来，那面试官就很怀疑你的水平和项目是否你做的了。 提高自己的编程水平。很多人都推荐剑指offer、leetcode。的确很不错的，我看现在剑指offer都更新到第二版了。至于编程提高到什么水平，剑指offer可以都看了，都搞明白了。我自己是刷了300道leetcode，刷了两遍左右的。我看周围的同学大都在100-200左右的样子，也都找到的非常不错的工作。我这里的经验是算法工程师的，很多公司的（比如滴滴、美团、百度、阿里等）算法工程岗位的面试，一场面试都是分三块，项目介绍、机器学习相关知识、编程题。一场面试一般就一道编程题。编程写不出来就基本挂了。当然面试也没有非要按一定套路的，要看面试官的，面试官喜欢问什么，我们就答什么。有的公司非常注重编程，比如微软，面试几乎全是编程题，而且编程的要求比较高。头条的编程题问的也很不容易。要是想找一般的互联网公司，感觉看完剑指offer、刷leetcode 100-200道就差不多了。当然这是我自己的判断，仅供参考，对此不负责任。还有建议是刷题的时候，要做一下笔记，经常回头看一看，我刷题的时候，使用某笔记（电子笔记类产品，避免广告嫌疑）记录一下题目，标签，解决思路与方法等。以后方便回顾。我自己就是经常看一看自己做的笔记，比每次都重头刷，思路清晰多了。建议尝试一下。 提高机器学习相关的技术水平。算法工程师一定会问机器学习相关的知识，这一块一定要好好的搞清楚明白了。我自己是看了李航老师的统计学习方法+部分prml+一些其他的学习。对这一块内容的理解应该是决定是否发大offer的关键。一般编程题都能写出来，写不出来就挂了。你对机器学习相关的理解深入程度、好的实习经历、好的论文、好的竞赛都是面试中的亮点。如果你其他方面都不能闪光的话，就把对技术的理解深入作为切入点，好好的专研下去吧。 本贴的目的在于对自己一年工作的总结与思考，同时希望能对学弟学妹们有所帮助欢迎讨论，如有不当，望指出。最后祝学弟学妹们都能有好的实习，收割大量的offer！如需转载，注明出处。 相关问题及我的想法：1.可以简单介绍一下你简历上有几个项目(我看文章里说是2个)，以及项目的大致工作吗？我自己写简历的时候也是蛮尴尬的，项目的确非常少。我自己只有一段小公司的实习经历，而且跟算法关系不大。除此之外，因为导师严格不让实习，就没有实习经历了。相对比较好的是，我们实验室都是做算法相关的，所以我把本科毕设写了进去。研究生期间导师也没有项目，所以我把我的研究方向（就是我自己研究方向做的实验，也是研究生毕设）写了进去。之前还有参加了一个小型的竞赛，所以这样子就凑够了四个项目。一般简历上大家写3-4个就够了，写太多也没太多意义。其中我的那个小公司实习面试的时候几乎没人问，所以在面实习的时候主要是问研究生相关的那个项目，那个小竞赛，还有本科毕设。后来校招的时候就把那个小公司实习给换成了滴滴的实习，把小公司实习给删了。找工作的时候大家都会问你的实习经历。当时的本科毕设做的的确比较水，又加上本科时水平菜，做的的确就那样。研究生的毕设做的还是挺认真的，还是有进步的空间。如果重新走的话，我这些都会更认真的做，找实习找工作还是非常有有用的。研一暑假是去实习的好机会，那时我们是按校历放假的，又有时间，我没有抓住。 2.项目在面试过程中的重要性是1/3左右吗？如果给项目、机器学习知识以及编程排序，应该是怎么样的？有没有项目经历不足，但用基础知识去平衡的情况？很多同学都会问项目相关问题。面试中的确会问项目，一般一个小时一场面试，常规的是20分钟聊项目，20分钟聊机器学习相关知识，最后20分钟编程并白纸算法。时间20分钟只是估计，只能说是大概。一般一面都是技术，二面可能技术也可能会随便聊。如上文所述，我也没有特别好看的项目。都是本科毕设，研究生毕设，小竞赛这样子的。大家也可以发挥聪明的脑子想想怎么写简历。至于重要性，都是非常重要的。有的面试官喜欢全程聊项目，聊着项目穿插着机器学习相关知识的提问。之前找实习的时候，很多人阿里的一面就是拿着简历聊项目及相关知识。这也涉及一个很大的问题，就是面试到底要面什么？分解起来就是要面什么内容，想考察什么能力呢？或者说一个算法工程师对这些的要求是什么呢？很直白的，很基本的就是编程，算法的理解，工程实践能力，还会考察数学知识，脑筋急转弯等，甚至更加深入的会考察你的知识的广度，深度，还有随机应变的能力。对于编程，我认为最低要求是别让编程拖你的后腿，一般来说，每场面试都会有一个编程题，一般套路是先讲思路，再写代码。代码最好bug-free。即使没有bug-free，经过提醒想明白也是可以的。大部分都会让分析下你的算法时间复杂度，空间复杂度，并能说明为什么是这个时间复杂度。很多都会追求最优解，所以这也给刷题一个提示，刷题不是ac就可以了，而是要追求最优解。编程这块可以自己把握，千万不要只追求编程，不管其他的。这多个方面要平衡下时间和精力。我之前有一段时间整天刷题，有段时间整天看算法，感觉都不是很好。最好能平衡下来，不让任何一方面去拉后腿。很多公司来说，编程会考察，但是通过的大家都给出最优解，差别也没那么大，所以多多掌握机器学习相关的算法吧，目前我是这么看的。关于工程实践，也就项目。做的项目，都是工程实现的一部分，如果什么项目都没做过，我们是面试官，我们也会对这样的面试者不放心的。像很好的实习经历，很多实验室或导师不放实习的情况下，实习经历很多同学都没有。如果有实习机会，一定要把握住。比如研一下的暑假，那个假期如果放假时间比较长的话，可以找个短期的实习，体验一下。有些如ACM大神，可能很多人是到不了这个层次的。对于多数同学来说，的确没有太好的实习经历。解决办法是如果条件允许，就去找一段实习。如果条件不允许，可以在学校好好的学好相关的机器学习理论，做好自己的课题。有些同学参加一些竞赛，也是可以的。至于排名，肯定是越靠前越好。我自己做的那个小竞赛也没拿到太好的名次。根据我的面试来看，面试官或者hr可能会问面试排名，也可能不问。但是排名并不是重点，重点是你能讲清楚你的项目中，你做了什么工作，思考出了哪些思路，都用了哪些方法，具体的工作及收获是什么。他们看到你的项目，重点还是想通过你的项目，看清楚你的能力和实力。至于这些竞赛能不能写到简历上，当然完全是可以的。机器学习算法相关的理解。很多人说，算法工程师日常工作是调参，调模型。调参和调用模型还是要理解其中的原理，才能向正确的方向走。对算法的考察，重点会考察你基本功的理解，比如svm lr的基本原理，各种分类的评价准则，l1 l2的区别等。再拓展的就是对算法广度的理解，最基本的就是对常用的一些算法有深入的理解就够了。其他相关的，算法工程师要掌握的很多，数学相关的知识用到的很多，比如矩阵，概率论相关的。比如滴滴之前问的求A和B的乘得到的矩阵C的秩，面试官聊完告诉我，这道题的重点就是考察一下数学相关的知识，因为算法工程师日常会用到很多数学知识。再比如，美团内推面试一面的时候，还问了一个概率题，我忘了写了，是这样的。一个村子特别热爱男孩，最初村子里男女平衡，但是夫妻生孩子时，如果生到女儿就会一直生，直到生到男孩为止，问，多年后，男女比例是多少?还问了两个人轮流投硬币，直到有人投到正面为赢。问先手和后手赢的概率多少。两个题很类似。到最后，无论是聊项目，还是机器学习相关算法，都是要考察你的整个的思维能力，基本功的能力。如果没有很好的项目，就从实习、竞赛、自己的实验室项目、自己的研究课题等方面找找灵感。重点还是自己对这些知识的掌握程度还有编程的能力。 3.如果没有好看的项目，简历上如何形成项目呢？或者是在目前这个紧急的阶段，可以临时做一些什么事情补救？类似问题：你做的项目是实验室的项目吗？实验室本身不是研究机器学习方向的，没有项目怎么办呢？这个问题，跟前面的有些类似。就从实习、竞赛、自己的实验室项目、研究课题等方面找灵感。我一同学，他也是通信相关方向，转nlp。就先找了一个小一点的公司实习，然后校招去了tmd中的一家。项目不是越多越好，我之前见有人简历上写了7个项目，写满了。然而这也没太多意义。简历上除了项目，可以写一下个人技能之类的，突出下自己的优点。我自己也是项目不够多，凑的。或许其他学长学姐有独特的项目技巧，我不太知道。 4.我最近参加了一个XX的比赛，但最后的名次可能不太好（猜测前10%吧，比赛没结束，最近疯狂被超，但感觉自己已经没有思路了），这种经历可以往简历上写吗？当然可以了。如前文所述，项目竞赛之类的写上简历，如果有好看的名次，肯定是好的，没有也没关系。前10%已经很不错了。我以为，面试官更看重的还是通过这个竞赛，体现了你对哪些知识的运用，自己提供这个竞赛的解决方案和思路是什么。重点还是考察你的能力和水准在哪里。如果一个竞赛，你拿到了特别靠前的名次，但面试的时候，什么也讲不出来，都是靠随便调参得来的，没有东西可以讲，还是非常尴尬的。 刷题上，笔记也做了分析也做了，但没几天前面的就感觉一点儿也想不起来了类似：很担心编程题啊。虽然在刷题，但是感觉现场还是不一定能bug-free啊，刷题好难啊编程很多考察的还在easy，偏medium的水准。当然也看公司看部门，有些公司就是看中编程，比如微软，那你想去就好好搞好编程就好了。对于很多公司来说，编程知识考察的一部分，一般都是medium水准。也有部分公司或部门考察hard的。刷题的时候，多做笔记，经常回顾，多找同学讨论。一道题，多多的思考，多多的去讨论。思路这个东西，有时候同学的理解和见识能很大的帮助你的成长。的确会忘记，但是你可以把一类题目总结下方法，或者看别人总结的方法。这个要多练习，多总结，多交流。我也确实没特别好的方法。但是我觉得经常回顾与思考，经常练习，肯定会对编程有所提升的。我自己看了cpp primer的一半和算法导论的一半左右，基本都看了算是两边多一点吧。剑指offer倒是看了四五遍。leetcode我做的题也基本在两遍以上吧。面试的时候，除了编程，有时候也会考察cpp的基本知识（如果你用cpp的话），比如虚函数的概念，struct与class的区别，指针，static等。 从你的经历来看，无论是基础的机器学习知识还是编程题（从我刷leetcode和剑指offer来看，感觉都是medium和hard的题目）的掌握来看，感觉师兄很历害，也拿到了很多sp的offer。但是在阿里的面试仅仅是因为投的较晚的原因导致的吗？是否还有其他的因素和注意的点。因为想去杭州，所以对阿里格外的关注。不仅仅是因为投的晚。我想说的是投的晚是非常重要的一个影响因素。我投的时候，我周围的同学都已经面过二面或者三面了，也就是说基本都已经结束了。所以错过内推阶段，加上内推消耗很多名额，转正也消耗了很多名额，最后留给激烈的校招的名额不多。最根本的还是自己的简历和自己的能力不够优秀或者说不足。如果能力非常强，什么时候都有机会嘛。但是对于很多人，包括我认识一些非常牛的的大神来说，还是要把握好机会，能事半功倍。我这里的建议是把握时间和机会，提前好好准备，不要犹豫。我校招投的蚂蚁金服，蚂蚁金服相对阿里其他部门来说是非常难进的部门，今年蚂蚁金服的实习貌似容易一些，我没投蚂蚁实习，校招也不容易吧。想去杭州的话，将来实习可以直接申请阿里杭州的实习，转正直接转本部门就好了。这样的一个坏处就是将来校招的时候，你可能在杭州实习，如果来北京面试其他公司，可能会不会太方便。但是你有了那边的offer，麻烦点也就那么回事。虽然有的公司内部也可以转到杭州，我觉得还是直接转会好一点。阿里的面试，根据其他同学的反馈，除了简历、项目、编程外，也会问一些数学或脑筋急转弯的问题。关于数学概率论的简单的一道题，两个随机变量X Y都符合均匀分布，那X+Y符合什么分布呢？看你想去哪个部门，多跟那里的学长学姐交流，可以针对性的准备下。 关于项目的问题。从你分享的内容来看，感觉项目占的不是很多，这一块是否占的比例比较小。因为我在实验室没做什么项目，只是学了一些相关的大数据处理工具，所以项目这一块很缺，最近打了一些比赛，这个用来充实项目是否够。关于项目，基本每场面试都会问吧。有的我可能没写要问项目。70%的面试都会先问问项目，很多都会一个或者两个。也有的会简单的问，有的深入的问。看面试官的风格。竞赛可以充实项目。只要你的竞赛跟你要找的岗位匹配的上。 你觉得公司对 数据挖掘比赛 看重的程度怎样？除非很好很知名的比赛你拿到特别好的名词，一般来说，更注重要考察的是你的知识的理解，全局把控，提出的解决方案等相关的能力。因为我自己的竞赛比较小，所以都是谈论技术相关的。其他大神的nb竞赛我也不知道怎么样的。 补充小米还问了对于类别不平衡的样本怎么处理。搜狗，对于分类使用auc f值等，对于预测问题的标准怎么评判？新浪，问了bias-variance分解问题。美团，搜狗，xgboost调哪些参数，哪些参数有效果，训练时间，是想问下具体过程。百度，svm lr损失函数对比，对噪声敏感性。还有问为何random forest能够降低误差的。新浪还问了，先验相关的，哪种先验分别对应l1 l2，还有高斯的先验是什么。阿里，熵是什么。]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>实习</tag>
        <tag>校招</tag>
        <tag>面试经验</tag>
        <tag>面经</tag>
        <tag>AI一丝一毫</tag>
      </tags>
  </entry>
</search>
